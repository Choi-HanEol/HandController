{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":372,"status":"ok","timestamp":1683712935506,"user":{"displayName":"최한얼","userId":"07324773000759146887"},"user_tz":-540},"id":"w__4_0hrF_hn"},"outputs":[{"ename":"ImportError","evalue":"\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"c:\\Users\\Haneol\\anaconda3\\envs\\handscontroller\\python.exe\"\n  * The NumPy version is: \"1.24.2\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: No module named 'numpy.core._multiarray_umath'\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\Haneol\\anaconda3\\envs\\handscontroller\\lib\\site-packages\\numpy\\core\\__init__.py:23\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m multiarray\n\u001b[0;32m     24\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n","File \u001b[1;32mc:\\Users\\Haneol\\anaconda3\\envs\\handscontroller\\lib\\site-packages\\numpy\\core\\multiarray.py:10\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfunctools\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m overrides\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _multiarray_umath\n","File \u001b[1;32mc:\\Users\\Haneol\\anaconda3\\envs\\handscontroller\\lib\\site-packages\\numpy\\core\\overrides.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_multiarray_umath\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     add_docstring, implement_array_function, _get_implementing_args)\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_inspect\u001b[39;00m \u001b[39mimport\u001b[39;00m getargspec\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m\n","File \u001b[1;32mc:\\Users\\Haneol\\anaconda3\\envs\\handscontroller\\lib\\site-packages\\numpy\\__init__.py:141\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m# Allow distributors to run custom init code\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _distributor_init\n\u001b[1;32m--> 141\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m core\n\u001b[0;32m    142\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m compat\n","File \u001b[1;32mc:\\Users\\Haneol\\anaconda3\\envs\\handscontroller\\lib\\site-packages\\numpy\\core\\__init__.py:49\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[39mIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39m\"\"\"\u001b[39m \u001b[39m%\u001b[39m (sys\u001b[39m.\u001b[39mversion_info[\u001b[39m0\u001b[39m], sys\u001b[39m.\u001b[39mversion_info[\u001b[39m1\u001b[39m], sys\u001b[39m.\u001b[39mexecutable,\n\u001b[0;32m     48\u001b[0m         __version__, exc)\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[0;32m     50\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     \u001b[39mfor\u001b[39;00m envkey \u001b[39min\u001b[39;00m env_added:\n","\u001b[1;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"c:\\Users\\Haneol\\anaconda3\\envs\\handscontroller\\python.exe\"\n  * The NumPy version is: \"1.24.2\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: No module named 'numpy.core._multiarray_umath'\n"]}],"source":["import numpy as np\n","import os\n","\n","os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n","os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1145,"status":"ok","timestamp":1683713127717,"user":{"displayName":"최한얼","userId":"07324773000759146887"},"user_tz":-540},"id":"pDduT_VKF_hu","outputId":"1f24f575-c57b-4337-a4c5-a6ae852a1ada"},"outputs":[{"data":{"text/plain":["(383, 30, 100)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["actions = [\n","    'handshake'\n","]\n","\n","data = np.concatenate([\n","    np.load('dataset/seq_handshake_1683712328.npy'),\n","], axis=0)\n","\n","data.shape"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1583,"status":"ok","timestamp":1683713135620,"user":{"displayName":"최한얼","userId":"07324773000759146887"},"user_tz":-540},"id":"XdwrcieZF_hw","outputId":"daf29162-795f-4a3a-b7b8-84368536cab1"},"outputs":[{"name":"stdout","output_type":"stream","text":["(383, 30, 99)\n","(383,)\n"]}],"source":["x_data = data[:, :, :-1]\n","labels = data[:, 0, -1]\n","\n","print(x_data.shape)\n","print(labels.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2675,"status":"ok","timestamp":1683713141015,"user":{"displayName":"최한얼","userId":"07324773000759146887"},"user_tz":-540},"id":"pr0r3VN_F_hx","outputId":"edb32a9c-ca3a-48fb-b537-fbe1c32df3e7"},"outputs":[{"data":{"text/plain":["(383, 1)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from tensorflow.keras.utils import to_categorical\n","\n","y_data = to_categorical(labels, num_classes=len(actions))\n","y_data.shape"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":693,"status":"ok","timestamp":1683713146272,"user":{"displayName":"최한얼","userId":"07324773000759146887"},"user_tz":-540},"id":"DJFszGg4F_hy","outputId":"89ec69bc-edc3-400c-bf18-e3e93e0dce85"},"outputs":[{"name":"stdout","output_type":"stream","text":["(344, 30, 99) (344, 1)\n","(39, 30, 99) (39, 1)\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","x_data = x_data.astype(np.float32)\n","y_data = y_data.astype(np.float32)\n","\n","x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.1, random_state=2021)\n","\n","print(x_train.shape, y_train.shape)\n","print(x_val.shape, y_val.shape)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":460,"status":"ok","timestamp":1683713220917,"user":{"displayName":"최한얼","userId":"07324773000759146887"},"user_tz":-540},"id":"rDDe2nKBF_hy","outputId":"2ac399a7-c2c5-4703-f081-81d0d824c9ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm (LSTM)                 (None, 64)                41984     \n","                                                                 \n"," dense (Dense)               (None, 32)                2080      \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 44,097\n","Trainable params: 44,097\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","\n","model = Sequential([\n","    LSTM(64, activation='relu', input_shape=x_train.shape[1:3]),\n","    Dense(32, activation='relu'),\n","    Dense(len(actions), activation='softmax')\n","])\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n","model.summary()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84412,"status":"ok","timestamp":1683713310315,"user":{"displayName":"최한얼","userId":"07324773000759146887"},"user_tz":-540},"id":"VZLxZG6IF_h0","outputId":"49c2e879-ea05-4fb0-f011-5099521f3bab","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/200\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n","  return dispatch_target(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 1: val_acc improved from -inf to 1.00000, saving model to models/model.h5\n","11/11 [==============================] - 2s 74ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 2/200\n","10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 2: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 32ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 3/200\n","10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 3: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 4/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 4: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 5/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 5: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 6/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 6: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 7/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 7: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 8/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 8: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 9/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 9: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 10/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 10: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 11/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 11: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 12/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 12: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 13/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 13: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 14/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 14: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 15/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 15: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 16/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 16: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 17/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 17: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 18/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 18: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 19/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 19: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 20/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 20: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 21/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 21: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 22/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 22: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 23/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 23: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 24/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 24: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 25/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 25: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 26/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 26: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 27/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 27: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 28/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 28: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 29/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 29: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 30/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 30: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 31/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 31: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 32/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 32: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 33/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 33: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 34/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 34: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 35/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 35: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 36/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 36: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 37/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 37: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 38/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 38: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 39/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 39: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 40/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 40: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 41/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 41: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 42/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 42: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 43/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 43: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 44/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 44: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 45/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 45: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 46/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 46: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 47/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 47: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 48/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 48: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 49/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 49: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 50/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 50: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 51/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 51: val_acc did not improve from 1.00000\n","\n","Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 0.0010\n","Epoch 52/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 52: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 53/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 53: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 54/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 54: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 55/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 55: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 56/200\n","10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 56: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 57/200\n","10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 57: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 58/200\n","10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 58: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 59/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 59: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 60/200\n","10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 60: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 61/200\n","10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 61: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 62/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 62: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 63/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 63: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 64/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 64: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 65/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 65: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 66/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 66: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 67/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 67: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 68/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 68: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 69/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 69: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 70/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 70: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 71/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 71: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 72/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 72: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 73/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 73: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 74/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 74: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 75/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 75: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 76/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 76: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 77/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 77: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 78/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 78: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 79/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 79: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 80/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 80: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 81/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 81: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 82/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 82: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 83/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 83: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 84/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 84: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 85/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 85: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 86/200\n","11/11 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 86: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 87/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 87: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 88/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 88: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 89/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 89: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 90/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 90: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 91/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 91: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 92/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 92: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 93/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 93: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 94/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 94: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 95/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 95: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 96/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 96: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 97/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 97: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 98/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 98: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 99/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 99: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 100/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 100: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 101/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 101: val_acc did not improve from 1.00000\n","\n","Epoch 101: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 5.0000e-04\n","Epoch 102/200\n","11/11 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 102: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 103/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 103: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 104/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 104: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 105/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 105: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 106/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 106: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 107/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 107: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 108/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 108: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 109/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 109: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 110/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 110: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 111/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 111: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 112/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 112: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 113/200\n","10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 113: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 114/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 114: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 115/200\n","11/11 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 115: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 116/200\n","10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 116: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 117/200\n","10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 117: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 118/200\n","10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 118: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 119/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 119: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 120/200\n","10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 120: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 121/200\n","11/11 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 121: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 122/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 122: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 123/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 123: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 124/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 124: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 125/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 125: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 126/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 126: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 127/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 127: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 128/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 128: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 129/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 129: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 130/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 130: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 131/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 131: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 132/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 132: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 133/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 133: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 134/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 134: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 135/200\n","11/11 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 135: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 136/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 136: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 137/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 137: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 138/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 138: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 139/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 139: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 140/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 140: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 141/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 141: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 142/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 142: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 143/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 143: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 144/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 144: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 145/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 145: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 146/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 146: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 147/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 147: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 148/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 148: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 149/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 149: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 150/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 150: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 151/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 151: val_acc did not improve from 1.00000\n","\n","Epoch 151: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 2.5000e-04\n","Epoch 152/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 152: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 153/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 153: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 154/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 154: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 155/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 155: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 156/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 156: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 157/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 157: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 158/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 158: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 159/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 159: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 160/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 160: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 161/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 161: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 162/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 162: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 163/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 163: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 164/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 164: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 165/200\n","10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 165: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 166/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 166: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 167/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 167: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 168/200\n","11/11 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 168: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 169/200\n","11/11 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 169: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 170/200\n","11/11 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 170: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 171/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 171: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 172/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 172: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 173/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 173: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 174/200\n","10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 174: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 33ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 175/200\n","10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 175: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 176/200\n","10/11 [==========================>...] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 176: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 177/200\n","11/11 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 177: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 178/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 178: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 179/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 179: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 180/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 180: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 181/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 181: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 182/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 182: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 183/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 183: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 184/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 184: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 185/200\n","11/11 [==============================] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 185: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 186/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 186: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 187/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 187: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 188/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 188: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 189/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 189: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 190/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 190: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 191/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 191: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 192/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 192: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 193/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 193: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 194/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 194: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 195/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 195: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 196/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 196: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 197/200\n"," 8/11 [====================>.........] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 197: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 198/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 198: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 199/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 199: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n","Epoch 200/200\n"," 9/11 [=======================>......] - ETA: 0s - loss: 0.0000e+00 - acc: 1.0000\n","Epoch 200: val_acc did not improve from 1.00000\n","11/11 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000 - lr: 1.2500e-04\n"]}],"source":["from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","\n","history = model.fit(\n","    x_train,\n","    y_train,\n","    validation_data=(x_val, y_val),\n","    epochs=200,\n","    callbacks=[\n","        ModelCheckpoint('models/model.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n","        ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')\n","    ]\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":712},"executionInfo":{"elapsed":1486,"status":"ok","timestamp":1683713348554,"user":{"displayName":"최한얼","userId":"07324773000759146887"},"user_tz":-540},"id":"aqUZb_hiF_h1","outputId":"659ca4f6-2793-4fe9-8720-e8b6f87ddd82"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1600x1000 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","\n","fig, loss_ax = plt.subplots(figsize=(16, 10))\n","acc_ax = loss_ax.twinx()\n","\n","loss_ax.plot(history.history['loss'], 'y', label='train loss')\n","loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n","loss_ax.set_xlabel('epoch')\n","loss_ax.set_ylabel('loss')\n","loss_ax.legend(loc='upper left')\n","\n","acc_ax.plot(history.history['acc'], 'b', label='train acc')\n","acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n","acc_ax.set_ylabel('accuracy')\n","acc_ax.legend(loc='upper left')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":749,"status":"ok","timestamp":1683713390951,"user":{"displayName":"최한얼","userId":"07324773000759146887"},"user_tz":-540},"id":"7KSNCjXLF_h2","outputId":"b617dec8-65fa-4b8f-cf57-36d103ba881b"},"outputs":[{"name":"stdout","output_type":"stream","text":["2/2 [==============================] - 0s 7ms/step\n"]},{"data":{"text/plain":["array([[[ 0,  0],\n","        [ 0, 39]]])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import multilabel_confusion_matrix\n","from tensorflow.keras.models import load_model\n","\n","model = load_model('models/model.h5')\n","\n","y_pred = model.predict(x_val)\n","\n","multilabel_confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-2L13nqTF_h3"},"outputs":[],"source":["\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}
